{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import json\n",
    "from docx import Document as Doc\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "import io\n",
    "import zipfile\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1448a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docx_images(file):\n",
    "    images = []\n",
    "    doc_zip = zipfile.ZipFile(file)\n",
    "    for name in doc_zip.namelist():\n",
    "        if name.startswith(\"word/media\"):\n",
    "            image_data = doc_zip.read(name)\n",
    "            image_bytes = io.BytesIO(image_data)\n",
    "            images.append((name, Image.open(image_bytes)))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "219c88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_and_ocr(images):\n",
    "    final_image_data = []\n",
    "    for image in images:\n",
    "        img_array = np.array(image[1])\n",
    "        img_bw = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "        final_image = cv2.GaussianBlur(img_bw, (3,3), 0)\n",
    "\n",
    "        extracted_image_data = pytesseract.image_to_string(final_image)\n",
    "        final_image_data.append((image[0], extracted_image_data))\n",
    "    \n",
    "    return final_image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0952ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_read_file(file):\n",
    "    content = file.read()\n",
    "    text = \"\"\n",
    "    if file.name.endswith(\".docx\"):\n",
    "        byte_content = io.BytesIO(content)\n",
    "        doc = Doc(byte_content)\n",
    "        paras = [para.text for para in doc.paragraphs]\n",
    "        tables = []\n",
    "\n",
    "        images = extract_docx_images(byte_content)\n",
    "        if images:\n",
    "            image_data = process_image_and_ocr(images)\n",
    "\n",
    "        image_text = \"\\n\".join(f\"{image[0]}\\n{image[1]}\\n\\n\" for image in image_data)\n",
    "\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                row_text = [cell.text.strip() for cell in row.cells]\n",
    "                tables.append(\"\\t\".join(row_text))\n",
    "        text = \"\\n\".join(tables + paras + [image_text])\n",
    "        return text\n",
    "\n",
    "    elif file.name.endswith(\".pdf\"):\n",
    "        file.seek(0)\n",
    "        content = file.read()\n",
    "        doc = fitz.open(stream=content, filetype=\"pdf\")\n",
    "        text = \"\\n\".join(page.get_text() for page in doc)\n",
    "        images = [page.get_images(full=True) for page in doc]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting file into chunks\n",
    "def split_pdf_to_chunks(file):\n",
    "    text = get_and_read_file(file)\n",
    "    splitter = SemanticChunker(embeddings=HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\"))\n",
    "    chunks = splitter.split_text(text)\n",
    "    data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        segment_data = {\"chunk_number\": i, \"chunk_content\": chunk}\n",
    "        data.append(segment_data)\n",
    "    return data\n",
    "\n",
    "def split_docx_to_chunks(file):\n",
    "    text = get_and_read_file(file)\n",
    "    splitter = SemanticChunker(embeddings=HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\"))\n",
    "    chunks = splitter.split_text(text)\n",
    "    data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        segment_data = {\"chunk_number\": i, \"chunk_content\": chunk}\n",
    "        data.append(segment_data)\n",
    "    return data\n",
    "\n",
    "def split_excel_to_chunks(file):\n",
    "    if file.name.endswith(\".xlsx\"):\n",
    "        content = file.read()\n",
    "        df = pd.read_excel(io.BytesIO(content), engine=\"openpyxl\")\n",
    "        headers = df.columns.tolist()\n",
    "        rows = df.values.tolist()\n",
    "        chunks = []\n",
    "        for i in range(len(rows)):\n",
    "            chunk = {\n",
    "                \"chunk_number\": i,\n",
    "                \"chunk_content\": []\n",
    "            }\n",
    "            for j in range(len(headers)):\n",
    "                value = rows[i][j]\n",
    "                if pd.isna(value):\n",
    "                    value = \"\"\n",
    "                elif isinstance(value, (pd.Timestamp, pd.NaT.__class__)):\n",
    "                    value = str(value)\n",
    "                elif isinstance(value, (datetime.datetime, datetime.date)):\n",
    "                    value = str(value)\n",
    "                chunk[\"chunk_content\"].append({headers[j]: value})\n",
    "            chunks.append(chunk)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(file, file_name):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        data = split_pdf_to_chunks(file)\n",
    "    elif file_name.endswith(\".docx\"):\n",
    "        data = split_docx_to_chunks(file)\n",
    "    elif file_name.endswith(\".xlsx\"):\n",
    "        data = split_excel_to_chunks(file)\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Not a '.docx' or '.pdf' file.\")\n",
    "        return []\n",
    "    chunks = [i[\"chunk_content\"] for i in data]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ec99d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "def save_chunks_to_file(chunks, file_name):\n",
    "    with open(f\"chunk_files/{file_name}_chunks.txt\", \"w\") as write_file:\n",
    "        json.dump(chunks, write_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0603b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_db():\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = Chroma(\n",
    "        collection_name=\"brd_collection\",\n",
    "        persist_directory=\"./my_db\",\n",
    "        embedding_function=embedding_model\n",
    "    )\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\":5})\n",
    "    return vectordb, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ae88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_vectordb(vectordb, chunks, file_name):\n",
    "    if not chunks:\n",
    "        return\n",
    "\n",
    "    docs = []\n",
    "    if file_name.endswith(\".xlsx\"):\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            content = \", \".join([\n",
    "                f\"{list(item.keys())[0]}: {list(item.values())[0]}\" for item in chunk[\"chunk_content\"]\n",
    "            ])\n",
    "            docs.append(Document(page_content=content, metadata={\"source\": file_name, \"chunk_number\": i}))\n",
    "    else:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            docs.append(Document(page_content=chunk, metadata={\"source\": file_name, \"chunk_number\": i}))\n",
    "    vectordb.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "816ed7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating RAG chain \n",
    "def create_qa_chain(retriever):\n",
    "\n",
    "    custom_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=(\n",
    "            \"Based only on the following context, answer the user's question. \"\n",
    "            \"If the answer is not present in the context, say 'Not found in the provided documents.'\\n\\n\"\n",
    "            \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    llm = Ollama(model=\"mistral\")\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": custom_prompt}\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3075b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query(query, qa_chain, file_name):\n",
    "    try:\n",
    "        if file_name:\n",
    "            return qa_chain.invoke({\"query\": query, \"filter\": {\"source\": file_name}})\n",
    "        else:\n",
    "            return qa_chain.invoke(query)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbac7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(vectordb, file):\n",
    "    chunks = get_chunks(file, file.name)\n",
    "    store_to_vectordb(vectordb, chunks, file.name)\n",
    "    save_chunks_to_file(chunks, file.name)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a096eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_db():\n",
    "    client = Chroma.Client()\n",
    "    collections = client.list_collections()\n",
    "\n",
    "    for col in collections:\n",
    "        client.delete_collection(name=col.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
